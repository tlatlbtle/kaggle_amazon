{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Prepare data\n",
    "\n",
    "将原始训练数据集,按照一定的ratio分割成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve,roc_auc_score,classification_report \n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_train_data(train_path,test_path,val_ratio):\n",
    "    train_data= pd.read_csv(train_path)\n",
    "    y_train = train_data.ACTION\n",
    "    X_train = train_data.drop(\"ACTION\",1)\n",
    "    \n",
    "    test_data= pd.read_csv(test_path)\n",
    "    X_test= test_data.drop(\"id\",1)\n",
    "    id_test=test_data.id\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, random_state=3)\n",
    "    print(\"Size of training data: \",len(X_train),\", Size of validation data: \",len(X_val))\n",
    "    return X_train, X_val, y_train,y_val,X_test,id_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Version1: Traditional machine learning method\n",
    "一些传统机器学习方法的测试,其中随机森林的结果最佳,而逻辑斯特回归效果欠佳,原因可能和数据集中标签为1的数据较多有关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data:  27853 , Size of validation data:  4916\n",
      "Random Forest:\n",
      "AUC:\n",
      "0.860190210829386\n",
      "Confusion Matrix:\n",
      "[[ 110  181]\n",
      " [  58 4567]]\n",
      "\n",
      "Logistic Regression:\n",
      "AUC:\n",
      "0.508703259960992\n",
      "Confusion Matrix:\n",
      "[[   0  291]\n",
      " [   0 4625]]\n",
      "\n",
      "Naive Bayes:\n",
      "AUC:\n",
      "0.5738333797715242\n",
      "Confusion Matrix:\n",
      "[[  18  273]\n",
      " [ 194 4431]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm,linear_model\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "def train_test(model,ans_path,X_train, X_val, y_train,y_val,X_test,id_test):\n",
    "    RFfit = model.fit(X_train , y_train)\n",
    "#     The AUC score of validation\n",
    "    pre_val = RFfit.predict_proba(X_val)\n",
    "    print(\"AUC:\")\n",
    "    print( roc_auc_score(y_val,pre_val[:,1]) )\n",
    "#      Confusion matrix\n",
    "    pre_val = RFfit.predict(X_val)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print( confusion_matrix(y_val,pre_val) )\n",
    "#   Saving predicting result\n",
    "    pre = RFfit.predict_proba(X_test)\n",
    "    ans=np.column_stack((id_test,pre))\n",
    "    np.savetxt(ans_path, ans, delimiter = ',')  \n",
    "    \n",
    "def main():\n",
    "    train_path=\"train.csv\"\n",
    "    test_path=\"test.csv\"\n",
    "    X_train, X_val, y_train,y_val,X_test,id_test=get_train_data(train_path,test_path,0.15)\n",
    "    #random forest\n",
    "    print(\"Random Forest:\")\n",
    "    forest = RandomForestClassifier(criterion='entropy',\n",
    "                                    n_estimators=1000,\n",
    "                                    random_state=1,\n",
    "                                    n_jobs=2)\n",
    "    train_test(forest,\"forest.csv\",X_train, X_val, y_train,y_val,X_test,id_test)\n",
    "    print(\"\\nLogistic Regression:\")\n",
    "    logist = linear_model.LogisticRegression()\n",
    "    train_test(logist,\"logist.csv\",X_train, X_val, y_train,y_val,X_test,id_test)\n",
    "    print(\"\\nNaive Bayes:\")\n",
    "    nb=GaussianNB()\n",
    "    train_test(nb,\"naive_bayes.csv\",X_train, X_val, y_train,y_val,X_test,id_test)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Verision2: Deep learning method\n",
    "利用pytorch和keras两种框架尝试了,可以在validation上达到0.94的准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 pytorch版本\n",
    "Tricks:\n",
    "\n",
    "(1)这里尝试了几种loss,目前使用的是Binary Cross Entropy(BCEloss).\n",
    "\n",
    "(2)optimizer使用的是Adam, learning rate为1e-5\n",
    "\n",
    "(3)在loss上加了权重约束,由于数据集中\n",
    "30872 are 1\n",
    "1897 are 0\n",
    "因此给标签为0和1的样本,loss权重分别设置为2和0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data:  26215 , Size of validation data:  6554\n",
      "[2.1845833333333333, 0.5303459437588509]\n",
      "Begin training!,lenth of training dataset is 26215\n",
      "Epoch: 0; Iters: 408; loss: 11.6188 ; accuracy of validation: 0.5285 ; loss_val: 8.1185 ; time elasped 1.47.\n",
      "Epoch: 1; Iters: 408; loss: 8.5586 ; accuracy of validation: 0.6454 ; loss_val: 6.7672 ; time elasped 2.69.\n",
      "Epoch: 2; Iters: 408; loss: 7.8113 ; accuracy of validation: 0.7309 ; loss_val: 5.7922 ; time elasped 1.29.\n",
      "Epoch: 3; Iters: 408; loss: 6.8388 ; accuracy of validation: 0.7890 ; loss_val: 5.1666 ; time elasped 1.45.\n",
      "Epoch: 4; Iters: 408; loss: 4.0795 ; accuracy of validation: 0.8351 ; loss_val: 4.6950 ; time elasped 1.46.\n",
      "Epoch: 5; Iters: 408; loss: 5.5615 ; accuracy of validation: 0.8651 ; loss_val: 4.3531 ; time elasped 1.48.\n",
      "Epoch: 6; Iters: 408; loss: 5.5469 ; accuracy of validation: 0.8789 ; loss_val: 4.2750 ; time elasped 1.47.\n",
      "Epoch: 7; Iters: 408; loss: 5.0536 ; accuracy of validation: 0.8892 ; loss_val: 4.1812 ; time elasped 1.30.\n",
      "Epoch: 8; Iters: 408; loss: 3.8273 ; accuracy of validation: 0.9028 ; loss_val: 3.9788 ; time elasped 1.34.\n",
      "Epoch: 9; Iters: 408; loss: 5.4027 ; accuracy of validation: 0.9139 ; loss_val: 3.9016 ; time elasped 1.45.\n",
      "Epoch: 10; Iters: 408; loss: 4.2306 ; accuracy of validation: 0.9173 ; loss_val: 3.8698 ; time elasped 1.32.\n",
      "Epoch: 11; Iters: 408; loss: 5.1587 ; accuracy of validation: 0.9213 ; loss_val: 3.7904 ; time elasped 1.47.\n",
      "Epoch: 12; Iters: 408; loss: 5.1774 ; accuracy of validation: 0.9213 ; loss_val: 3.8154 ; time elasped 1.45.\n",
      "Epoch: 13; Iters: 408; loss: 5.4027 ; accuracy of validation: 0.9263 ; loss_val: 3.7717 ; time elasped 1.42.\n",
      "Epoch: 14; Iters: 408; loss: 5.1737 ; accuracy of validation: 0.9266 ; loss_val: 3.7356 ; time elasped 1.44.\n",
      "Epoch: 15; Iters: 408; loss: 5.1737 ; accuracy of validation: 0.9274 ; loss_val: 3.7344 ; time elasped 1.45.\n",
      "Epoch: 16; Iters: 408; loss: 4.9448 ; accuracy of validation: 0.9284 ; loss_val: 3.7624 ; time elasped 1.39.\n",
      "Epoch: 17; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9312 ; loss_val: 3.7061 ; time elasped 1.45.\n",
      "Epoch: 18; Iters: 408; loss: 5.0733 ; accuracy of validation: 0.9326 ; loss_val: 3.6986 ; time elasped 1.46.\n",
      "Epoch: 19; Iters: 408; loss: 4.9448 ; accuracy of validation: 0.9332 ; loss_val: 3.6880 ; time elasped 1.46.\n",
      "Epoch: 20; Iters: 408; loss: 4.9448 ; accuracy of validation: 0.9333 ; loss_val: 3.7134 ; time elasped 1.46.\n",
      "Epoch: 21; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9339 ; loss_val: 3.7023 ; time elasped 1.44.\n",
      "Epoch: 22; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9355 ; loss_val: 3.6808 ; time elasped 1.45.\n",
      "Epoch: 23; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9359 ; loss_val: 3.7004 ; time elasped 1.45.\n",
      "Epoch: 24; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9371 ; loss_val: 3.6613 ; time elasped 2.72.\n",
      "Epoch: 25; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9362 ; loss_val: 3.6801 ; time elasped 1.43.\n",
      "Epoch: 26; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9368 ; loss_val: 3.6647 ; time elasped 1.45.\n",
      "Epoch: 27; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9362 ; loss_val: 3.6765 ; time elasped 1.44.\n",
      "Epoch: 28; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9373 ; loss_val: 3.6823 ; time elasped 1.47.\n",
      "Epoch: 29; Iters: 408; loss: 4.9442 ; accuracy of validation: 0.9382 ; loss_val: 3.6629 ; time elasped 1.35.\n",
      "Epoch: 30; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9377 ; loss_val: 3.6640 ; time elasped 1.45.\n",
      "Epoch: 31; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9379 ; loss_val: 3.6720 ; time elasped 1.42.\n",
      "Epoch: 32; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9377 ; loss_val: 3.6721 ; time elasped 1.30.\n",
      "Epoch: 33; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9388 ; loss_val: 3.6477 ; time elasped 1.45.\n",
      "Epoch: 34; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9385 ; loss_val: 3.6630 ; time elasped 1.43.\n",
      "Epoch: 35; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9388 ; loss_val: 3.6577 ; time elasped 1.41.\n",
      "Epoch: 36; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9379 ; loss_val: 3.6664 ; time elasped 1.44.\n",
      "Epoch: 37; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9377 ; loss_val: 3.6717 ; time elasped 1.41.\n",
      "Epoch: 38; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9376 ; loss_val: 3.6674 ; time elasped 1.48.\n",
      "Epoch: 39; Iters: 408; loss: 4.7158 ; accuracy of validation: 0.9387 ; loss_val: 3.6633 ; time elasped 1.42.\n",
      "Ans saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import sklearn\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def validation_acc(model,X_val,y_val,use_gpu,class_weight):\n",
    "    lenth=len(X_val)\n",
    "    val_ans=np.zeros(y_val.shape)\n",
    "    score=model(X_val).squeeze()\n",
    "    score_=score.data.cpu().numpy()\n",
    "    y_val_=y_val.data.cpu().numpy()\n",
    "    val_ans[score_>=0.5]=1\n",
    "    val_ans=val_ans.astype(\"int64\")\n",
    "    correct = (y_val_ == val_ans).sum()\n",
    "    accuracy = 1.0*correct / lenth\n",
    "    #calculate AUC\n",
    "    auc=roc_auc_score(y_val_,val_ans)\n",
    "    \n",
    "    weight = class_weight[y_val.long()]\n",
    "    loss_fn = torch.nn.BCELoss(weight=weight)\n",
    "    loss_val=loss_fn(score,y_val)\n",
    "    return accuracy,auc,loss_val\n",
    "\n",
    "def convert2onehot(label,config):\n",
    "    class_num = config[\"y_dim\"]\n",
    "    batch_size = len(label)\n",
    "    label = torch.LongTensor(batch_size, 1).random_() % class_num\n",
    "    one_hot = torch.zeros(batch_size, class_num).scatter_(1, label, 1)\n",
    "    \n",
    "    return one_hot.numpy()\n",
    "\n",
    "def train(X_train, X_val, y_train,y_val,config):\n",
    "    train_path=config[\"train_path\"]\n",
    "    test_path=config[\"test_path\"]\n",
    "    ans_path=config[\"ans_path\"]\n",
    "    model_path=config[\"model_path\"]\n",
    "    batch_size=config[\"batch_size\"]\n",
    "    hidden_dim =config[\"hidden_dim\"]\n",
    "    X_dim=config[\"X_dim\"]\n",
    "    y_dim =config[\"y_dim\"]\n",
    "    epoch=config[\"epoch\"]\n",
    "    use_gpu=config[\"use_gpu\"]\n",
    "    use_one_hot=config[\"use_one_hot\"]\n",
    "    it_per_epoch=int( len(X_train)/batch_size )\n",
    "    \n",
    "    if use_gpu:\n",
    "        X_train = Variable( torch.FloatTensor( torch.from_numpy(X_train).numpy() ).cuda() )\n",
    "        y_train = Variable( torch.FloatTensor( torch.from_numpy(y_train).numpy() ).cuda() )\n",
    "        X_val = Variable( torch.FloatTensor( torch.from_numpy(X_val).numpy() ).cuda() )\n",
    "        y_val = Variable( torch.FloatTensor( torch.from_numpy(y_val).numpy() ).cuda() )\n",
    "    else:\n",
    "        X_train = Variable( torch.FloatTensor( torch.from_numpy(X).numpy() ) )\n",
    "        y_train = Variable( torch.FloatTensor( torch.from_numpy(y).numpy() ) )\n",
    "        X_val = Variable( torch.FloatTensor( torch.from_numpy(X_val).numpy() ) )\n",
    "        y_val = Variable( torch.FloatTensor( torch.from_numpy(y_val).numpy() ) )\n",
    "    \n",
    "    \n",
    "    D = torch.nn.Sequential(\n",
    "        torch.nn.Linear(X_dim, hidden_dim[0]),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(hidden_dim[0], hidden_dim[1]),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(hidden_dim[1], hidden_dim[2]),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(hidden_dim[2], hidden_dim[3]),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(hidden_dim[3], hidden_dim[4]),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(hidden_dim[4], hidden_dim[5]),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(hidden_dim[5], 1),\n",
    "        torch.nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    #Optimizer\n",
    "    D_solver = optim.Adam(D.parameters(), lr=1e-6)\n",
    "#     D_solver = optim.SGD(D.parameters(), lr=1e-5, momentum=0.9)\n",
    "    \n",
    "    if use_gpu: \n",
    "        D=D.cuda()\n",
    "    #Loss\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    \n",
    "    #The positive samples are less \n",
    "#     30872 are 1\n",
    "#     1897 are 0\n",
    "    c_weight = list(sklearn.utils.class_weight.compute_class_weight('balanced', \n",
    "                                                               np.unique(y_train.data.cpu().numpy()), \n",
    "                                                                    y_train.data.cpu().numpy()) )\n",
    "    c_weight[0]/=4\n",
    "    print(c_weight)\n",
    "    if use_gpu:\n",
    "        class_weight = Variable(torch.FloatTensor(c_weight).cuda())\n",
    "    else:\n",
    "        class_weight = Variable(torch.FloatTensor(c_weight) )\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"Begin training!,lenth of training dataset is %d\"%(len(X_train)))\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        train_data_begin=0\n",
    "        start=time.time()\n",
    "        for it in range(it_per_epoch):\n",
    "            # Sample data\n",
    "            X = X_train[train_data_begin:train_data_begin+batch_size]\n",
    "            y = y_train[train_data_begin:train_data_begin+batch_size]\n",
    "            \n",
    "            train_data_begin=train_data_begin+batch_size\n",
    "            \n",
    "            score=D(X).squeeze()\n",
    "            weight = class_weight[y.long()]\n",
    "            loss_fn = torch.nn.BCELoss(weight=weight)\n",
    "            loss =loss_fn(score,y)\n",
    "            \n",
    "            loss.backward()\n",
    "            D_solver.step()\n",
    "\n",
    "            D.zero_grad()\n",
    "\n",
    "            end=time.time()\n",
    "        \n",
    "        #Calculating precision of validation dataset\n",
    "        val_acc,auc,loss_val=validation_acc(D,X_val,y_val,use_gpu,class_weight)\n",
    "        \n",
    "        print('Epoch: %d; Iters: %d; loss: %.4f ; accuracy of validation: %.4f ; loss_val: %.4f ; time elasped %.2f.'\n",
    "                      %(epoch,it, loss.data[0],val_acc,loss_val.data[0],end-start))\n",
    "#         print(\"The first 5 answers are: {}\".format( score.data.cpu().numpy()[:5]) )\n",
    "        \n",
    "        if epoch%10==0:\n",
    "            torch.save(D, model_path+'%d_D_model.pkl'%epoch)\n",
    "\n",
    "def test(id_test,X_test,model_path,config):\n",
    "    use_gpu=config[\"use_gpu\"]\n",
    "    ans_path=config[\"ans_path\"]\n",
    "    model = torch.load(model_path)\n",
    "    if use_gpu:\n",
    "        X_test=Variable( torch.FloatTensor( torch.from_numpy(X_test).numpy() ).cuda())\n",
    "    else:\n",
    "        X_test=Variable( torch.FloatTensor( torch.from_numpy(X_test).numpy() ))\n",
    "    ans=model(X_test)\n",
    "    ans=np.column_stack((  id_test,ans.data.cpu().numpy()  ))\n",
    "    np.savetxt(ans_path, ans, delimiter = ',') \n",
    "    print(\"Ans saved.\")\n",
    "    \n",
    "def main():\n",
    "    config={\n",
    "        \"train_path\":\"train.csv\",\n",
    "        \"test_path\":\"test.csv\",\n",
    "        \"ans_path\":'ans.csv',\n",
    "        \"model_path\":\"model/\",\n",
    "        \"batch_size\":64,\n",
    "        \"hidden_dim\" : [16,64,128,256,128,64],\n",
    "        \"X_dim\" :9,\n",
    "        \"y_dim\" : 2,\n",
    "        \"epoch\" :40,\n",
    "        \"use_gpu\":True,\n",
    "        \"use_one_hot\":False\n",
    "    }\n",
    "    X_train, X_val, y_train,y_val,X_test,id_test=get_train_data(config[\"train_path\"],config[\"test_path\"],0.2)\n",
    "    \n",
    "    train(X_train.values,X_val.values, y_train.values,y_val.values,config)\n",
    "    \n",
    "    test(id_test,X_test.values,config[\"model_path\"]+\"40_D_model.pkl\",config)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 keras版本\n",
    "Tricks:\n",
    "\n",
    "(1)loss依旧是Cross Entropy.\n",
    "\n",
    "(2)optimizer使用的是Adam, learning rate为5e-5\n",
    "\n",
    "(3)在loss上加了权重约束,loss权重分别设置为8和0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data:  24576 , Size of validation data:  8193\n",
      "[8.75213675 0.53029518]\n",
      "Train on 24576 samples, validate on 8193 samples\n",
      "Epoch 1/40\n",
      "24576/24576 [==============================] - 11s 446us/step - loss: 0.2200 - acc: 0.9429 - val_loss: 0.2282 - val_acc: 0.9398\n",
      "Epoch 2/40\n",
      "24576/24576 [==============================] - 8s 341us/step - loss: 0.2201 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 3/40\n",
      "24576/24576 [==============================] - 8s 343us/step - loss: 0.2200 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 4/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2200 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 5/40\n",
      "24576/24576 [==============================] - 10s 396us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 6/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2202 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 7/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2200 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 8/40\n",
      "24576/24576 [==============================] - 8s 341us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 9/40\n",
      "24576/24576 [==============================] - 10s 396us/step - loss: 0.2199 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 10/40\n",
      "24576/24576 [==============================] - 8s 344us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 11/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2200 - acc: 0.9429 - val_loss: 0.2281 - val_acc: 0.9398\n",
      "Epoch 12/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2201 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 13/40\n",
      "24576/24576 [==============================] - 10s 395us/step - loss: 0.2201 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 14/40\n",
      "24576/24576 [==============================] - 8s 341us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 15/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 16/40\n",
      "24576/24576 [==============================] - 10s 399us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 17/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 18/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 19/40\n",
      "24576/24576 [==============================] - 8s 343us/step - loss: 0.2199 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 20/40\n",
      "24576/24576 [==============================] - 10s 397us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 21/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 22/40\n",
      "24576/24576 [==============================] - 8s 344us/step - loss: 0.2200 - acc: 0.9429 - val_loss: 0.2280 - val_acc: 0.9398\n",
      "Epoch 23/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 24/40\n",
      "24576/24576 [==============================] - 10s 397us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 25/40\n",
      "24576/24576 [==============================] - 8s 341us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 26/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2199 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 27/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 28/40\n",
      "24576/24576 [==============================] - 10s 397us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 29/40\n",
      "24576/24576 [==============================] - 8s 342us/step - loss: 0.2199 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 30/40\n",
      "24576/24576 [==============================] - 8s 343us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 31/40\n",
      "24576/24576 [==============================] - 8s 344us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 32/40\n",
      "24576/24576 [==============================] - 10s 398us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 33/40\n",
      "24576/24576 [==============================] - 8s 341us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 34/40\n",
      "24576/24576 [==============================] - 8s 341us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 35/40\n",
      "24576/24576 [==============================] - 8s 343us/step - loss: 0.2196 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 36/40\n",
      "24576/24576 [==============================] - 10s 396us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 37/40\n",
      "24576/24576 [==============================] - 8s 345us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 38/40\n",
      "24576/24576 [==============================] - 8s 340us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 39/40\n",
      "24576/24576 [==============================] - 8s 344us/step - loss: 0.2197 - acc: 0.9429 - val_loss: 0.2279 - val_acc: 0.9398\n",
      "Epoch 40/40\n",
      "24576/24576 [==============================] - 10s 396us/step - loss: 0.2198 - acc: 0.9429 - val_loss: 0.2278 - val_acc: 0.9398\n",
      "8193/8193 [==============================] - 1s 112us/step\n",
      "validation accuracy: [0.22815047834697375, 0.9398266813133163]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input,Dense, Dropout, Activation, Flatten,advanced_activations\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import class_weight\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create the base model\n",
    "def dense_block(x,filters):\n",
    "    x=Dense(filters,kernel_initializer='glorot_normal',kernel_regularizer=regularizers.l2(0))(x)\n",
    "    x=Activation('relu')(x)\n",
    "#     x=advanced_activations.LeakyReLU(alpha=0.3)(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def train(X_train,X_val, y_train,y_val,config):\n",
    "    batch_size=config[\"batch_size\"]\n",
    "    num_classes =config[\"num_classes\"]\n",
    "    epochs=config[\"epoch\"]\n",
    "    train_path=config[\"train_path\"]\n",
    "    hidden_dim =config[\"hidden_dim\"]\n",
    "    use_gpu=config[\"use_gpu\"]\n",
    "    use_one_hot=config[\"use_one_hot\"]\n",
    "    filepath=config[\"model_path\"]\n",
    "    load_pre_model=config[\"load_pre_model\"]\n",
    "    \n",
    "    main_input = Input(shape=X_train.shape[1:],name='input')\n",
    "    x=main_input\n",
    "    \n",
    "    for i in range(len(hidden_dim)):\n",
    "        x=dense_block(x,hidden_dim[i])\n",
    "\n",
    "    predictions = Dense(num_classes, activation='sigmoid',name='main_output',kernel_initializer='glorot_normal',\n",
    "                       kernel_regularizer=regularizers.l2(0))(x)\n",
    "    \n",
    "    if load_pre_model:\n",
    "        model=load_model(filepath) \n",
    "    else:\n",
    "        model = Model(inputs=main_input, outputs=predictions)\n",
    "\n",
    "    opt=Adam(lr=5e-5, beta_1=0.99, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    c_weight = class_weight.compute_class_weight('balanced', np.unique(y_train[:,1]), y_train[:,1])\n",
    "#     c_weight = {0 : 26,1: 1.}\n",
    "    print(c_weight)\n",
    "    \n",
    "    model.fit(X_train,y_train,\n",
    "              validation_data=(X_val,y_val),class_weight=c_weight,\n",
    "              epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[ModelCheckpoint(filepath,monitor='val_acc',\n",
    "                                         verbose=0,save_best_only=True,mode='auto')])\n",
    "    \n",
    "    \n",
    "def test(id_test,X_test,X_val,y_val,config):\n",
    "    model = load_model(config[\"model_path\"]) \n",
    "    scores = model.evaluate(X_val,y_val, verbose=1)\n",
    "    print(\"validation accuracy:\",scores)\n",
    "    \n",
    "    predict=model.predict(X_test)\n",
    "    ans=np.column_stack((  id_test,predict[:,1]  ))\n",
    "    np.savetxt(config[\"ans_path\"], ans, delimiter = ',')   \n",
    "    \n",
    "def main():\n",
    "    config={\n",
    "        \"train_path\":\"train.csv\",\n",
    "        \"test_path\":\"test.csv\",\n",
    "        \"ans_path\":'ans_keras.csv',\n",
    "        \"model_path\":\"model/keras.h5\",\n",
    "        \"batch_size\":32,\n",
    "        \"hidden_dim\" : [16,64,128,256,128,64],\n",
    "        \"X_dim\" :9,\n",
    "        \"num_classes\" : 2,\n",
    "        \"epoch\" :40,\n",
    "        \"use_gpu\":True,\n",
    "        \"use_one_hot\":False,\n",
    "        \"load_pre_model\":True\n",
    "    }\n",
    "    X_train, X_val, y_train,y_val,X_test,id_test=get_train_data(config[\"train_path\"],config[\"test_path\"],0.25)\n",
    "    \n",
    "    y_train = to_categorical(y_train, config[\"num_classes\"])\n",
    "    y_val =  to_categorical(y_val, config[\"num_classes\"])\n",
    "    \n",
    "    train(X_train,X_val, y_train,y_val,config)\n",
    "    \n",
    "    test(id_test,X_test,X_val,y_val,config)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
